{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OriginalJapaneseTextGenGen2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V4IuE6LHN5Sp"
      ],
      "mount_file_id": "1qAXTcK42jen-VR1cnM0f_jPx5Sie5uLO",
      "authorship_tag": "ABX9TyPYMPNzuU5HKaJaIoTsvtG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidr-1123/LyricGeneratorJapanese/blob/main/OriginalJapaneseTextGenGen2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYkJMXvvLgvR"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKtPZ-NR4ZhO"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import MeCab\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM\n",
        "import os\n",
        "from google.colab import files \n",
        "\n",
        "files.upload()\n",
        "text = open('aimyon.txt', 'r', encoding='utf-8').read()\n",
        "text = text\n",
        "table = str.maketrans({\n",
        "    '\\u3000': '',\n",
        "    '…': '。',\n",
        "    '”': '」',\n",
        "    '“': '「',\n",
        "    ',': '、',\n",
        "    '.': '。'\n",
        "})\n",
        "text = text.translate(table)\n",
        "\n",
        "wakati = MeCab.Tagger('-Owakati')\n",
        "words = wakati.parse(text).split(' ')\n",
        "print(words)\n",
        "\n",
        "vocab = sorted(set(words))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([char2idx[c] for c in words])\n",
        "\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "\n",
        "def int_to_text(ints):\n",
        "    try:\n",
        "        ints = ints.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ''.join(idx2char[ints])\n",
        "\n",
        "\n",
        "print(int_to_text(text_as_int[:5]))\n",
        "\n",
        "seq_length = 10  # length of sequence for a training example\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry\n",
        "\n",
        "for x, y in dataset.take(2):\n",
        "    print(\"\\n\\nEXAMPLE\\n\")\n",
        "    print(\"INPUT\")\n",
        "    print(int_to_text(x))\n",
        "    print(\"\\nOUTPUT\")\n",
        "    print(int_to_text(y))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 1024\n",
        "RNN_UNITS = 2048 * 2\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints_aimyon'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_freq=int(2300 * 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtxmpggG4qd8"
      },
      "source": [
        "history = model.fit(data, epochs=201, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9WlGjqezFxZ"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMYW-4ieZIZ9"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "    file = open('Generated_lyrics_aimyon.txt', 'a', encoding='utf-8')\n",
        "    num_generate = 25\n",
        "\n",
        "    wordlist = wakati.parse(start_string).split(\",\")\n",
        "    for word in wordlist:\n",
        "      word.replace('\\n', '')\n",
        "    try:\n",
        "      # Converting our start string to numbers (vectorizing)\n",
        "      input_eval = [char2idx[s] for s in words]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "      # Empty string to store our results\n",
        "      text_generated = []\n",
        "\n",
        "      # Low temperatures results in more predictable text.\n",
        "      # Higher temperatures results in more surprising text.\n",
        "      # Experiment to find the best setting.\n",
        "      temperature = 1.0\n",
        "\n",
        "      # Here batch size == 1\n",
        "      model.reset_states()\n",
        "      for i in range(num_generate):\n",
        "          predictions = model(input_eval)\n",
        "          # remove the batch dimension\n",
        "\n",
        "          predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "          # using a categorical distribution to predict the character returned by the model\n",
        "          predictions = predictions / temperature\n",
        "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "          # We pass the predicted character as the next input to the model\n",
        "          # along with the previous hidden state\n",
        "          input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "          text_generated.append(idx2char[predicted_id])\n",
        "    except KeyError:\n",
        "      print('別の言葉を選んでくれ')\n",
        "    file.write(start_string + ''.join(text_generated))\n",
        "    file.write(\"\\n\")\n",
        "    file.close()\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32T-lQRpDaaZ",
        "outputId": "7404a8ef-1865-4d5e-87a2-bd6b332cdc48"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "inp = wakati.parse(inp).replace(' ', '')\n",
        "inp = inp.replace('\\n', '')\n",
        "table = str.maketrans({\n",
        "    '\\u3000': '',\n",
        "    '…': '。',\n",
        "    '”': '」',\n",
        "    '“': '「',\n",
        "    ',': '、',\n",
        "    '.': '。'\n",
        "})\n",
        "inp = inp.translate(table)\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: 大好き\n",
            "大好き理想の表現ってのは同情だけで成り立ってんのかなありったけの水をちょうだい白いしれない余裕のある\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4IuE6LHN5Sp"
      },
      "source": [
        "#### **Email Sending**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9pXjth_I1qy"
      },
      "source": [
        "import imaplib\n",
        "import email\n",
        "from email.header import decode_header, make_header\n",
        "from time import sleep\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import formataddr\n",
        "from email.header import Header\n",
        "import smtplib\n",
        "\n",
        "while True:\n",
        "  user = 'david1123ramsay@gmail.com'\n",
        "  pswrd = 'trpvmnloupsggejj'\n",
        "  port = 993\n",
        "  host = 'imap.gmail.com'\n",
        "\n",
        "  mail = imaplib.IMAP4_SSL(host=host, port=port)\n",
        "  mail.login(user, pswrd)\n",
        "\n",
        "  mail.select('INBOX')\n",
        "  _, data = mail.search(None, \"UNSEEN\")\n",
        "\n",
        "  inbox = data[0].split()\n",
        "  msg_list = []\n",
        "  for num in inbox:\n",
        "      typ, b = mail.fetch(num, '(RFC822)')\n",
        "      # noinspection PyUnresolvedReferences\n",
        "      msg = email.message_from_bytes(b[0][1])\n",
        "      msg_list.append(msg)\n",
        "\n",
        "  mail.close()\n",
        "  mail.logout()\n",
        "\n",
        "  my_inbox = []\n",
        "\n",
        "  for msg in msg_list:\n",
        "      message_data = {}\n",
        "      headers = ['Subject', 'From', 'To', 'Date']\n",
        "      for header in headers:\n",
        "          message_data[header] = str(make_header(decode_header(msg[header])))\n",
        "\n",
        "      if msg.is_multipart() is False:\n",
        "          payload = msg.get_payload(decode=True)\n",
        "          charset = msg.get_content_charset()\n",
        "          if charset is not None:\n",
        "              payload = payload.decode(charset, 'ignore')\n",
        "      else:\n",
        "          for part in msg.walk():\n",
        "              if part.get_content_type() == 'text/plain':\n",
        "                  payload = part.get_payload(decode=True)\n",
        "                  if payload is None:\n",
        "                      continue\n",
        "                  charset = part.get_content_charset()\n",
        "                  if charset is not None:\n",
        "                      payload = payload.decode(charset, 'ignore')\n",
        "                  message_data['Body'] = payload\n",
        "              elif part.get_content_type() == 'text/html':\n",
        "                  payload = part.get_payload(decode=True)\n",
        "                  if payload is None:\n",
        "                      continue\n",
        "                  charset = part.get_content_charset()\n",
        "                  if charset is not None:\n",
        "                      payload = payload.decode(charset, 'ignore')\n",
        "                  message_data['HTML'] = payload\n",
        "      my_inbox.append(message_data)\n",
        "\n",
        "  for message in my_inbox:\n",
        "    print(message['Body'])\n",
        "  try:\n",
        "    inp = my_inbox[-1]['Body']\n",
        "    inp = wakati.parse(inp).replace(' ', '')\n",
        "    inp = inp.replace('\\n', '')\n",
        "    table = str.maketrans({\n",
        "        '\\u3000': '',\n",
        "        '…': '。',\n",
        "        '”': '」',\n",
        "        '“': '「',\n",
        "        ',': '、',\n",
        "          '.': '。'\n",
        "    })\n",
        "    inp = inp.translate(table)\n",
        "    print(inp)\n",
        "    output = (generate_text(model, inp))\n",
        "    print(output)\n",
        "\n",
        "\n",
        "    def send(subject, message, recipients, from_email='david1123ramsay@gmail.com', password='trpvmnloupsggejj'):\n",
        "        # SMTP認証情報\n",
        "\n",
        "        # MIMEの作成\n",
        "        msg = MIMEText(message + \"\\n\\n\\nThis message was sent using an automated python script.\")\n",
        "        msg[\"Subject\"] = subject\n",
        "        msg[\"To\"] = (', ').join((recipients.split(',')))\n",
        "        msg[\"From\"] = formataddr((str(Header(\"PythonBot\", \"utf-8\")), from_email))\n",
        "\n",
        "\n",
        "        # メール送信処理\n",
        "        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
        "        server.starttls()\n",
        "        server.login(from_email, password)\n",
        "        server.send_message(msg)\n",
        "        server.quit()\n",
        "\n",
        "    send('Lyric Response Service', output, 'dramsay530@gmail.com')\n",
        "    print('sent')\n",
        "    sleep(30)\n",
        "  except IndexError:\n",
        "    sleep(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34yqVEXEO90C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}