{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OriginalJapaneseTextGenGen2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOykUhMak1VBZECB7GtfqfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidr-1123/LyricGeneratorJapanese/blob/main/OriginalJapaneseTextGenGen2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYkJMXvvLgvR"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKtPZ-NR4ZhO"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import MeCab\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM\n",
        "import os\n",
        "from google.colab import files \n",
        "\n",
        "files.upload()\n",
        "text = open('aimyon.txt', 'r', encoding='utf-8').read()\n",
        "text = text\n",
        "table = str.maketrans({\n",
        "    '\\u3000': '',\n",
        "    '…': '。',\n",
        "    '”': '」',\n",
        "    '“': '「',\n",
        "    ',': '、',\n",
        "    '.': '。'\n",
        "})\n",
        "text = text.translate(table)\n",
        "\n",
        "wakati = MeCab.Tagger('-Owakati')\n",
        "words = wakati.parse(text).split(' ')\n",
        "print(words)\n",
        "\n",
        "vocab = sorted(set(words))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([char2idx[c] for c in words])\n",
        "\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "\n",
        "def int_to_text(ints):\n",
        "    try:\n",
        "        ints = ints.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ''.join(idx2char[ints])\n",
        "\n",
        "\n",
        "print(int_to_text(text_as_int[:5]))\n",
        "\n",
        "seq_length = 10  # length of sequence for a training example\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry\n",
        "\n",
        "for x, y in dataset.take(2):\n",
        "    print(\"\\n\\nEXAMPLE\\n\")\n",
        "    print(\"INPUT\")\n",
        "    print(int_to_text(x))\n",
        "    print(\"\\nOUTPUT\")\n",
        "    print(int_to_text(y))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 1024\n",
        "RNN_UNITS = 2048 * 2\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints_aimyon'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_freq=int(2300 * 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtxmpggG4qd8"
      },
      "source": [
        "history = model.fit(data, epochs=201, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9WlGjqezFxZ"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMYW-4ieZIZ9"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "    file = open('Generated_lyrics_aimyon.txt', 'a', encoding='utf-8')\n",
        "    num_generate = 25\n",
        "\n",
        "    wordlist = wakati.parse(start_string).split(\",\")\n",
        "    for word in wordlist:\n",
        "      word.replace('\\n', '')\n",
        "    try:\n",
        "      # Converting our start string to numbers (vectorizing)\n",
        "      input_eval = [char2idx[s] for s in words]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "      # Empty string to store our results\n",
        "      text_generated = []\n",
        "\n",
        "      # Low temperatures results in more predictable text.\n",
        "      # Higher temperatures results in more surprising text.\n",
        "      # Experiment to find the best setting.\n",
        "      temperature = 1.0\n",
        "\n",
        "      # Here batch size == 1\n",
        "      model.reset_states()\n",
        "      for i in range(num_generate):\n",
        "          predictions = model(input_eval)\n",
        "          # remove the batch dimension\n",
        "\n",
        "          predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "          # using a categorical distribution to predict the character returned by the model\n",
        "          predictions = predictions / temperature\n",
        "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "          # We pass the predicted character as the next input to the model\n",
        "          # along with the previous hidden state\n",
        "          input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "          text_generated.append(idx2char[predicted_id])\n",
        "    except KeyError:\n",
        "      print('別の言葉を選んでくれ')\n",
        "    file.write(start_string + ''.join(text_generated))\n",
        "    file.write(\"\\n\")\n",
        "    file.close()\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32T-lQRpDaaZ"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "inp = wakati.parse(inp).replace(' ', '')\n",
        "inp = inp.replace('\\n', '')\n",
        "table = str.maketrans({\n",
        "    '\\u3000': '',\n",
        "    '…': '。',\n",
        "    '”': '」',\n",
        "    '“': '「',\n",
        "    ',': '、',\n",
        "    '.': '。'\n",
        "})\n",
        "inp = inp.translate(table)\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}